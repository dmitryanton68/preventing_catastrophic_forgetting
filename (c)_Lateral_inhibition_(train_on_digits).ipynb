{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (c) Lateral inhibition    \n",
    "### Initial training on digits with radius inhibition: r1=5, r2=2, r3=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.cuda as cuda\n",
    "import torch.utils.data as data_utils\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "try:\n",
    "    import accimage\n",
    "except ImportError:\n",
    "    accimage = None\n",
    "\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "from SpykeTorch import snn\n",
    "from SpykeTorch import functional as sf\n",
    "from SpykeTorch import visualization as vis\n",
    "from SpykeTorch import utils\n",
    "\n",
    "import struct\n",
    "import glob\n",
    "import datetime\n",
    "import re\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "\n",
    "class MozafariMNIST2018(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(MozafariMNIST2018, self).__init__()\n",
    "\n",
    "        self.conv1 = snn.Convolution(6, 30, 5, 0.8, 0.05)\n",
    "        self.conv1_t = 15\n",
    "        self.k1 = 5\n",
    "        self.r1 = 5 #  radius inhibition r1\n",
    "\n",
    "        self.conv2 = snn.Convolution(30, 250, 3, 0.8, 0.05)\n",
    "        self.conv2_t = 10\n",
    "        self.k2 = 8\n",
    "        self.r2 = 2 #  radius inhibition r2\n",
    "\n",
    "        self.conv3 = snn.Convolution(250, 200, 5, 0.8, 0.05)\n",
    "\n",
    "        self.stdp1 = snn.STDP(self.conv1, (0.004, -0.003))\n",
    "        self.stdp2 = snn.STDP(self.conv2, (0.004, -0.003))\n",
    "        self.stdp3 = snn.STDP(self.conv3, (0.004, -0.003), False, 0.2, 0.8)\n",
    "        self.anti_stdp3 = snn.STDP(self.conv3, (-0.004, 0.0005), False, 0.2, 0.8)\n",
    "        self.max_ap = Parameter(torch.Tensor([0.15]))\n",
    "\n",
    "        self.decision_map = []\n",
    "        for i in range(10):\n",
    "            self.decision_map.extend([i]*20)\n",
    "\n",
    "        self.ctx = {\"input_spikes\":None, \"potentials\":None, \"output_spikes\":None, \"winners\":None}\n",
    "        self.spk_cnt1 = 0\n",
    "        self.spk_cnt2 = 0\n",
    "\n",
    "    def forward(self, input, max_layer):\n",
    "        \n",
    "        input = sf.pad(input.float(), (2,2,2,2), 0)\n",
    "        \n",
    "        if self.training:\n",
    "            pot = self.conv1(input)\n",
    "            spk, pot = sf.fire(pot, self.conv1_t, True)\n",
    "            if max_layer == 1:\n",
    "                self.spk_cnt1 += 1\n",
    "                if self.spk_cnt1 >= 500:\n",
    "                    self.spk_cnt1 = 0\n",
    "                    ap = torch.tensor(self.stdp1.learning_rate[0][0].item(), device=self.stdp1.learning_rate[0][0].device) * 2\n",
    "                    ap = torch.min(ap, self.max_ap)\n",
    "                    an = ap * -0.75\n",
    "                    self.stdp1.update_all_learning_rate(ap.item(), an.item())\n",
    "                pot = sf.pointwise_inhibition(pot)\n",
    "                spk = pot.sign()\n",
    "                winners = sf.get_k_winners(pot, self.k1, self.r1, spk)\n",
    "                self.ctx[\"input_spikes\"] = input\n",
    "                self.ctx[\"potentials\"] = pot\n",
    "                self.ctx[\"output_spikes\"] = spk\n",
    "                self.ctx[\"winners\"] = winners\n",
    "                return spk, pot\n",
    "            \n",
    "            spk_in = sf.pad(sf.pooling(spk, 2, 2), (1,1,1,1))\n",
    "            pot = self.conv2(spk_in)\n",
    "            spk, pot = sf.fire(pot, self.conv2_t, True)\n",
    "            if max_layer == 2:\n",
    "                self.spk_cnt2 += 1\n",
    "                if self.spk_cnt2 >= 500:\n",
    "                    self.spk_cnt2 = 0\n",
    "                    ap = torch.tensor(self.stdp2.learning_rate[0][0].item(), device=self.stdp2.learning_rate[0][0].device) * 2\n",
    "                    ap = torch.min(ap, self.max_ap)\n",
    "                    an = ap * -0.75\n",
    "                    self.stdp2.update_all_learning_rate(ap.item(), an.item())\n",
    "                pot = sf.pointwise_inhibition(pot)\n",
    "                spk = pot.sign()\n",
    "                winners = sf.get_k_winners(pot, self.k2, self.r2, spk)\n",
    "                self.ctx[\"input_spikes\"] = spk_in\n",
    "                self.ctx[\"potentials\"] = pot\n",
    "                self.ctx[\"output_spikes\"] = spk\n",
    "                self.ctx[\"winners\"] = winners\n",
    "                return spk, pot\n",
    "            \n",
    "            spk_in = sf.pad(sf.pooling(spk, 3, 3), (2,2,2,2))\n",
    "            pot = self.conv3(spk_in)\n",
    "            spk = sf.fire(pot)\n",
    "            winners = sf.get_k_winners(pot, 1, 0, spk)\n",
    "            self.ctx[\"input_spikes\"] = spk_in\n",
    "            self.ctx[\"potentials\"] = pot\n",
    "            self.ctx[\"output_spikes\"] = spk\n",
    "            self.ctx[\"winners\"] = winners\n",
    "            output = -1\n",
    "            if len(winners) != 0:\n",
    "                output = self.decision_map[winners[0][0]]\n",
    "            return output\n",
    "        \n",
    "        else:\n",
    "            pot = self.conv1(input)\n",
    "            spk, pot = sf.fire(pot, self.conv1_t, True)\n",
    "            if max_layer == 1:\n",
    "                return spk, pot\n",
    "            \n",
    "            pot = self.conv2(sf.pad(sf.pooling(spk, 2, 2), (1,1,1,1)))\n",
    "            spk, pot = sf.fire(pot, self.conv2_t, True)\n",
    "            if max_layer == 2:\n",
    "                return spk, pot\n",
    "            pot = self.conv3(sf.pad(sf.pooling(spk, 3, 3), (2,2,2,2)))\n",
    "            spk = sf.fire(pot)\n",
    "            winners = sf.get_k_winners(pot, 1, 0, spk) #  radius inhibition r3\n",
    "            output = -1\n",
    "            if len(winners) != 0:\n",
    "                output = self.decision_map[winners[0][0]]\n",
    "            return output\n",
    "\n",
    "    def stdp(self, layer_idx):\n",
    "        if layer_idx == 1:\n",
    "            self.stdp1(self.ctx[\"input_spikes\"], self.ctx[\"potentials\"], self.ctx[\"output_spikes\"], self.ctx[\"winners\"])\n",
    "        if layer_idx == 2:\n",
    "            self.stdp2(self.ctx[\"input_spikes\"], self.ctx[\"potentials\"], self.ctx[\"output_spikes\"], self.ctx[\"winners\"])\n",
    "\n",
    "    def update_learning_rates(self, stdp_ap, stdp_an, anti_stdp_ap, anti_stdp_an):\n",
    "        self.stdp3.update_all_learning_rate(stdp_ap, stdp_an)\n",
    "        self.anti_stdp3.update_all_learning_rate(anti_stdp_an, anti_stdp_ap)\n",
    "\n",
    "    def reward(self):\n",
    "        self.stdp3(self.ctx[\"input_spikes\"], self.ctx[\"potentials\"], self.ctx[\"output_spikes\"], self.ctx[\"winners\"])\n",
    "\n",
    "    def punish(self):\n",
    "        self.anti_stdp3(self.ctx[\"input_spikes\"], self.ctx[\"potentials\"], self.ctx[\"output_spikes\"], self.ctx[\"winners\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train and test\n",
    "\n",
    "def train_unsupervise(network, data, layer_idx):\n",
    "    network.train()\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        data_in = data[i]\n",
    "        if use_cuda:\n",
    "            data_in = data_in.cuda()\n",
    "        network(data_in, layer_idx)\n",
    "        network.stdp(layer_idx)\n",
    "\n",
    "def train_rl(network, data, target):\n",
    "    network.train()\n",
    "    perf = np.array([0,0,0]) # correct, wrong, silence\n",
    "    for i in range(len(data)):\n",
    "        data_in = data[i]\n",
    "        target_in = target[i]\n",
    "        if use_cuda:\n",
    "            data_in = data_in.cuda()\n",
    "            target_in = target_in.cuda()\n",
    "        d = network(data_in, 3)\n",
    "        if d != -1:\n",
    "            if d == target_in:\n",
    "                perf[0]+=1\n",
    "                network.reward()\n",
    "            else:\n",
    "                perf[1]+=1\n",
    "                network.punish()\n",
    "        else:\n",
    "            perf[2]+=1\n",
    "    return perf/len(data)\n",
    "\n",
    "def test(network, data, target):\n",
    "    network.eval()\n",
    "    perf = np.array([0,0,0]) # correct, wrong, silence\n",
    "    for i in range(len(data)):\n",
    "        data_in = data[i]\n",
    "        target_in = target[i]\n",
    "        if use_cuda:\n",
    "            data_in = data_in.cuda()\n",
    "            target_in = target_in.cuda()\n",
    "        d = network(data_in, 3)\n",
    "        if d != -1:\n",
    "            if d == target_in:\n",
    "                perf[0]+=1\n",
    "            else:\n",
    "                perf[1]+=1\n",
    "        else:\n",
    "            perf[2]+=1\n",
    "    return perf/len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def curve_graph(parametr_set):\n",
    "\n",
    "    plt.subplots(figsize=(15, 5))\n",
    "\n",
    "    plt.plot(parametr_set['epoch'], parametr_set['train']*100, color='cyan', label='train')\n",
    "    plt.plot(parametr_set['epoch'], parametr_set['test']*100, color='blue', marker = 'o', label='test')\n",
    "    plt.plot(parametr_set['epoch'], parametr_set['test_previous']*100, linestyle = ':', color='red', label='test of previous images')\n",
    "    plt.xlabel('epochs', loc='right', fontsize=17)\n",
    "    plt.ylabel('accuracy, %',  loc='top', fontsize=17)\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the 3rd layer\n",
    "\n",
    "def third_layer(file_name_net, file_name_csv, adaptive_int, previous_epochs, epochs, \n",
    "                train_loader, test_loader, test_previous_loader,\n",
    "                model, apr, anr, app, anp, parametr_set, steps=None, percent=20, it_continues=False):  \n",
    "    \n",
    "    '''\n",
    "    file_name_net - name of file for saving state_dict of model\n",
    "    file_name_csv - name of file for saving parameters of model in each epoch\n",
    "    adaptive_int - learning rate parameter\n",
    "    previous_epochs - if before model had training in current period\n",
    "    it_continues - is it continue of 3-rd layer training or not (False or True)\n",
    "    percent - percent of moving weights (calculated from the number of high range weights)\n",
    "    '''\n",
    "\n",
    "    adaptive_min=0 \n",
    "\n",
    "    if not it_continues:\n",
    "\n",
    "        previous_epochs = 0\n",
    "        counter = 0\n",
    "\n",
    "        apr_adapt = ((1.0 - 1.0 / 10) * adaptive_int + adaptive_min) * apr\n",
    "        anr_adapt = ((1.0 - 1.0 / 10) * adaptive_int + adaptive_min) * anr\n",
    "        app_adapt = ((1.0 / 10) * adaptive_int + adaptive_min) * app\n",
    "        anp_adapt = ((1.0 / 10) * adaptive_int + adaptive_min) * anp\n",
    "        \n",
    "        best_train = np.array([0.0,0.0,0.0,0.0]) # correct, wrong, silence, epoch\n",
    "        best_test = np.array([0.0,0.0,0.0,0.0]) # correct, wrong, silence, epoch\n",
    "        best_test_previous = np.array([0.0,0.0,0.0,0.0]) # correct, wrong, silence, epoch\n",
    "\n",
    "    else:\n",
    "      \n",
    "        if len(parametr_set.loc[parametr_set['test'] == parametr_set['test'].max(), 'epoch']) == 1:\n",
    "            optim_index = int(parametr_set.loc[parametr_set['test'] == parametr_set['test'].max(), 'epoch'].item())\n",
    "        else:\n",
    "            optim_index = int(parametr_set.loc[parametr_set['test'] == parametr_set['test'].max(), 'epoch'].tolist()[-1])\n",
    "\n",
    "        if len(parametr_set.loc[parametr_set['train'] == parametr_set['train'].max(), 'epoch']) == 1:\n",
    "            best_train_index = int(parametr_set.loc[parametr_set['train'] == parametr_set['train'].max(), 'epoch'].item())\n",
    "        else:\n",
    "            best_train_index = int(parametr_set.loc[parametr_set['train'] == parametr_set['train'].max(), 'epoch'].tolist()[-1])\n",
    "\n",
    "        if len(parametr_set.loc[parametr_set['test_previous'] == parametr_set['test_previous'].max(), 'epoch']) == 1:\n",
    "            best_test_previous_index = int(parametr_set.loc[parametr_set['test_previous'] == parametr_set['test_previous'].max(), 'epoch'].item())\n",
    "        else:\n",
    "            best_test_previous_index = int(parametr_set.loc[parametr_set['test_previous'] == parametr_set['test_previous'].max(), 'epoch'].tolist()[-1])\n",
    "        \n",
    "        max_index = int(parametr_set.index.max())\n",
    "        counter = (max_index + 1)\n",
    "\n",
    "        param_best_train = parametr_set['train'].iloc[best_train_index]\n",
    "        param_best_test = parametr_set['test'].iloc[optim_index]\n",
    "        param_best_test_previous = parametr_set['test_previous'].iloc[best_test_previous_index]\n",
    "\n",
    "        apr_adapt = parametr_set['apr_adapt'].iloc[optim_index]\n",
    "        anr_adapt = parametr_set['anr_adapt'].iloc[optim_index]\n",
    "        app_adapt = parametr_set['app_adapt'].iloc[optim_index]\n",
    "        anp_adapt = parametr_set['anp_adapt'].iloc[optim_index]\n",
    "        \n",
    "        for i in range(len(mozafari.stdp3.learning_rate)):\n",
    "            mozafari.stdp3.learning_rate[i][0].fill_(parametr_set['stdp3.learning_rate[0]'].iloc[optim_index])\n",
    "            mozafari.stdp3.learning_rate[i][1].fill_(parametr_set['stdp3.learning_rate[1]'].iloc[optim_index])\n",
    "            mozafari.anti_stdp3.learning_rate[0][0].fill_(parametr_set['anti_stdp3.learning_rate[0]'].iloc[optim_index])\n",
    "            mozafari.anti_stdp3.learning_rate[0][0].fill_(parametr_set['anti_stdp3.learning_rate[1]'].iloc[optim_index])\n",
    "\n",
    "        best_train = np.array([param_best_train,1-param_best_train,0.0,best_train_index]) # correct, wrong, silence, epoch\n",
    "        best_test = np.array([param_best_test,1-param_best_test,0.0,optim_index]) # correct, wrong, silence, epoch\n",
    "        best_test_previous = np.array([param_best_test_previous,1-param_best_test_previous,0.0,best_test_previous_index]) # correct, wrong, silence, epoch\n",
    "    \n",
    "    # list of 3-rd layer weights\n",
    "\n",
    "    dim_0, dim_1, dim_2, dim_3 = tuple(mozafari.conv3.weight.size())\n",
    "    total_size = dim_0 * dim_1 * dim_2 * dim_3\n",
    "  \n",
    "    # indexes of weights\n",
    "    indexes_i = []    \n",
    "    indexes_j = []        \n",
    "    indexes_k = []        \n",
    "    indexes_m = []    \n",
    "    \n",
    "    # values of weights\n",
    "    item_values = []  \n",
    "    \n",
    "    for i in range(dim_0):\n",
    "        for j in range(dim_1):\n",
    "            for k in range(dim_2):\n",
    "                for m in range(dim_3):\n",
    "                    indexes_i.append(i)\n",
    "                    indexes_j.append(j)\n",
    "                    indexes_k.append(k)\n",
    "                    indexes_m.append(m)\n",
    "                    item_values.append(mozafari.conv3.weight[i][j][k][m].item())\n",
    "\n",
    "    indexes_dim_0 = pd.Series(indexes_i, name='dim_0') \n",
    "    indexes_dim_1 = pd.Series(indexes_j, name='dim_1')\n",
    "    indexes_dim_2 = pd.Series(indexes_k, name='dim_2')\n",
    "    indexes_dim_3 = pd.Series(indexes_m, name='dim_3')\n",
    "    item_values = pd.Series(item_values, name='value_0')\n",
    "            \n",
    "    conv3_data = pd.concat([item_values, indexes_dim_0, indexes_dim_1, indexes_dim_2, indexes_dim_3], axis=1)\n",
    "    \n",
    "    high_percent = 85 #percent of high range weights\n",
    "    percentile_value = np.percentile(item_values, high_percent)\n",
    "    \n",
    "    conv3_data['low_range_0'] = 0\n",
    "    conv3_data.loc[conv3_data['value_0'] < percentile_value,'low_range_0'] = 1\n",
    "    \n",
    "    try:\n",
    "        high_range_counter = conv3_data['low_range_0'].value_counts()[0] \n",
    "    except:\n",
    "        high_range_counter = 1\n",
    " \n",
    "    moving_quantity = int((percent/100)*high_range_counter) #quantity of moving items in each epoch\n",
    "    \n",
    "    if steps is None:\n",
    "        steps = int(total_size*high_percent/(100*moving_quantity))   #steps of weights moving \n",
    "    print(f'Weight moving will be during {steps} epochs')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        seconds_epoch_0 = time.time() \n",
    "        \n",
    "        print('-'*50)\n",
    "        print(\"Epoch #: \", epoch + previous_epochs)\n",
    "        \n",
    "        perf_train = np.array([0.0,0.0,0.0]) \n",
    "        \n",
    "        for data,targets in train_loader:\n",
    "                \n",
    "            if epoch < steps: \n",
    "                \n",
    "                print(f'Values of high range weights in epoch#{epoch} [{percentile_value :.3f}:0.800] (top {100-high_percent}%)')\n",
    "                low_range_indexes = list(conv3_data.index[conv3_data['low_range_'+str(epoch)] == 1])\n",
    "                moving_items = random.sample(low_range_indexes, np.minimum(moving_quantity, len(low_range_indexes)))\n",
    "                moving_indexes = conv3_data.loc[conv3_data.index.isin(moving_items)]\n",
    "\n",
    "                print(f'Quantity of moving points in epoch#{epoch + previous_epochs} = {len(moving_indexes.index)} items' \n",
    "                      f' ({len(moving_indexes.index)/(total_size-high_range_counter)*100 :.1f}% of moving points)')\n",
    "\n",
    "                for q in range(len(moving_indexes.index)):\n",
    "                    mozafari.conv3.weight \\\n",
    "                    [moving_indexes['dim_0'].iloc[q]][moving_indexes['dim_1'].iloc[q]][moving_indexes['dim_2'].iloc[q]][moving_indexes['dim_3'].iloc[q]]. \\\n",
    "                    fill_(np.random.normal(loc=0.8, scale=0.05))  \n",
    "              \n",
    "            perf_train_batch = train_rl(model, data, targets)\n",
    "    \n",
    "            if epoch < steps:  \n",
    "            \n",
    "                # new values of weights (after learning)\n",
    "                item_values = []       \n",
    "                for i in range(dim_0):\n",
    "                    for j in range(dim_1):\n",
    "                        for k in range(dim_2):\n",
    "                            for m in range(dim_3):\n",
    "                                item_values.append(mozafari.conv3.weight[i][j][k][m].item())\n",
    "            \n",
    "                item_values = pd.Series(item_values, name='value_'+str(epoch+1))\n",
    "                percentile_value = np.percentile(item_values, high_percent) #new cutting off high range weights\n",
    "                conv3_data = pd.concat([conv3_data, item_values], axis=1)\n",
    "                \n",
    "                conv3_data['low_range_'+str(epoch+1)] = 0\n",
    "                conv3_data.loc[conv3_data['value_'+str(epoch+1)] < percentile_value,'low_range_'+str(epoch+1)] = 1\n",
    "       \n",
    "            #update adaptive learning rates\n",
    "            apr_adapt = apr * (perf_train_batch[1] * adaptive_int + adaptive_min)\n",
    "            anr_adapt = anr * (perf_train_batch[1] * adaptive_int + adaptive_min)\n",
    "            app_adapt = app * (perf_train_batch[0] * adaptive_int + adaptive_min)\n",
    "            anp_adapt = anp * (perf_train_batch[0] * adaptive_int + adaptive_min)\n",
    "            parametr_set.loc[counter, 'epoch'] = epoch + previous_epochs\n",
    "            parametr_set.loc[counter, 'apr_adapt'] = apr_adapt\n",
    "            parametr_set.loc[counter, 'anr_adapt'] = anr_adapt\n",
    "            parametr_set.loc[counter, 'app_adapt'] = app_adapt\n",
    "            parametr_set.loc[counter, 'anp_adapt'] = anp_adapt\n",
    "            parametr_set.loc[counter, 'stdp3.learning_rate[0]'] = mozafari.stdp3.learning_rate[0][0].item()\n",
    "            parametr_set.loc[counter, 'stdp3.learning_rate[1]'] = mozafari.stdp3.learning_rate[0][1].item()\n",
    "            parametr_set.loc[counter, 'anti_stdp3.learning_rate[0]'] = mozafari.anti_stdp3.learning_rate[0][0].item()\n",
    "            parametr_set.loc[counter, 'anti_stdp3.learning_rate[1]'] = mozafari.anti_stdp3.learning_rate[0][1].item()\n",
    "            parametr_set.loc[counter, 'train'] = perf_train_batch[0]\n",
    "\n",
    "            model.update_learning_rates(apr_adapt, anr_adapt, app_adapt, anp_adapt)\n",
    "            perf_train += perf_train_batch\n",
    "            \n",
    "        perf_train /= len(train_loader)\n",
    "\n",
    "        if best_train[0] <= perf_train[0]:\n",
    "            best_train = np.append(perf_train, epoch + previous_epochs)\n",
    "        print(f\"Current Train: {perf_train[0]*100 :.2f}%\")\n",
    "\n",
    "        for data,targets in test_loader:\n",
    "            perf_test = test(model, data, targets)\n",
    "            parametr_set.loc[counter, 'test'] = perf_test[0]\n",
    "            if best_test[0] <= perf_test[0]:\n",
    "                best_test = np.append(perf_test, epoch + previous_epochs)\n",
    "                torch.save(model.state_dict(), file_name_net)\n",
    "            print(f\"Current Test: {perf_test[0]*100 :.2f}%\")\n",
    "\n",
    "        if isinstance(test_previous_loader, DataLoader):\n",
    "            for data,targets in test_previous_loader:\n",
    "                perf_test_previous = test(model, data, targets)\n",
    "                parametr_set.loc[counter, 'test_previous'] = perf_test_previous[0]\n",
    "                if best_test_previous[0] <= perf_test_previous[0]:\n",
    "                    best_test_previous = np.append(perf_test_previous, epoch + previous_epochs)\n",
    "                print(f\"Current Test Previous: {perf_test_previous[0]*100 :.2f}%\")\n",
    "                \n",
    "        else:\n",
    "            parametr_set.loc[counter, 'test_previous'] = 0\n",
    "            \n",
    "        counter += 1\n",
    "                                                 \n",
    "        seconds_epoch_1 = time.time()  \n",
    "        print(f'Operational time of epoch #{epoch + previous_epochs}: '\n",
    "                  f'{int((seconds_epoch_1 - seconds_epoch_0)//60)} min {int((seconds_epoch_1 - seconds_epoch_0)%60)} sec') \n",
    "    \n",
    "    parametr_set.to_csv(file_name_csv)\n",
    "    \n",
    "    print('=='*10, 'SUMMARY', '=='*10)\n",
    "    print(f\"        Best Train: {best_train[0]*100 :.2f}% on {best_train[3] :.0f} epoch\")\n",
    "    print(f\"         Best Test: {best_test[0]*100 :.2f}% on {best_test[3] :.0f} epoch\")\n",
    "    print(f\"Best Test Previous: {best_test_previous[0]*100 :.2f}% on {best_test_previous[3] :.0f} epoch\")\n",
    "    \n",
    "    return parametr_set, counter, (previous_epochs+epochs), apr, anr, app, anp, conv3_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for image transformation (see dataset)\n",
    "\n",
    "class S1C1Transform:\n",
    "    \n",
    "    def __init__(self, filter, PIL_type=False, timesteps = 15):\n",
    "        self.PIL_type = PIL_type\n",
    "        self.to_pil_image = transforms.ToPILImage()    \n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "        self.filter = filter\n",
    "        self.temporal_transform = utils.Intensity2Latency(timesteps)\n",
    "        self.cnt = 0\n",
    "        \n",
    "    def __call__(self, image):\n",
    "        if self.cnt % 10000 == 0:\n",
    "            print(f'{self.cnt} images')\n",
    "        if self.PIL_type:\n",
    "            image = self.to_pil_image(image)\n",
    "        self.cnt+=1\n",
    "        image = self.to_tensor(image) * 255\n",
    "        image.unsqueeze_(0)\n",
    "        image = self.filter(image)\n",
    "        image = sf.local_normalization(image, 8)\n",
    "        temporal_image = self.temporal_transform(image)\n",
    "        return temporal_image.sign().byte()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for image transformation (see dataset)\n",
    "\n",
    "kernels = [ utils.DoGKernel(3,3/9,6/9),\n",
    "            utils.DoGKernel(3,6/9,3/9),\n",
    "            utils.DoGKernel(7,7/9,14/9),\n",
    "            utils.DoGKernel(7,14/9,7/9),\n",
    "            utils.DoGKernel(13,13/9,26/9),\n",
    "            utils.DoGKernel(13,26/9,13/9)]\n",
    "\n",
    "filter = utils.Filter(kernels, padding = 6, thresholds = 50)\n",
    "\n",
    "s1c1 = S1C1Transform(filter)\n",
    "s1c1_PIL = S1C1Transform(filter, PIL_type=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for image transformation (see dataset)\n",
    "\n",
    "class CustomTensorDataset(Dataset):\n",
    "    \"\"\"TensorDataset with support of transforms.\"\"\"\n",
    "    \n",
    "    def __init__(self, tensors, transform=None):\n",
    "        assert all(tensors[0].size(0) == tensor.size(0) for tensor in tensors)\n",
    "        self.tensors = tensors\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.tensors[0][index]\n",
    "\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "\n",
    "        y = self.tensors[1][index]\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tensors[0].size(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set of capital letters (letters A, B, D, E, G, H, N, Q, R, S)\n",
    "24000 train images + 4000 test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set of 10 capital letters from EMNIST (A, B, D, E, G, H, N, Q, R, S)\n",
    "path = f'./data/EMNIST_own/capital_letters/'\n",
    "\n",
    "test_letter_labels = torch.load(f'{path}Mozafari_capital_letters_test_labels.pt', map_location=torch.device('cpu'))\n",
    "test_letters = torch.load(f'{path}Mozafari_capital_letters_test_images.pt', map_location=torch.device('cpu'))\n",
    "\n",
    "train_letter_labels = torch.load(f'{path}Mozafari_capital_letters_train_labels.pt', map_location=torch.device('cpu'))\n",
    "train_letters = torch.load(f'{path}Mozafari_capital_letters_train_images.pt', map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Element permutation\n",
    "\n",
    "train_order_l = torch.randperm(train_letter_labels.shape[0])\n",
    "test_order_l = torch.randperm(test_letter_labels.shape[0])\n",
    "\n",
    "train_letter_labels = train_letter_labels[train_order_l].view(train_letter_labels.size())\n",
    "train_letters = train_letters[train_order_l].view(train_letters.size())\n",
    "\n",
    "test_letter_labels = test_letter_labels[test_order_l].view(test_letter_labels.size())\n",
    "test_letters = test_letters[test_order_l].view(test_letters.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_letter_set = CustomTensorDataset(tensors=(train_letters, train_letter_labels), transform=s1c1_PIL)\n",
    "test_letter_set = CustomTensorDataset(tensors=(test_letters, test_letter_labels), transform=s1c1_PIL)\n",
    "\n",
    "train_letter_loader = DataLoader(train_letter_set, batch_size=len(train_letter_set))\n",
    "test_letter_loader = DataLoader(test_letter_set, batch_size=len(test_letter_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([24000]), torch.Size([4000]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_letter_labels.size(), test_letter_labels.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set of MNIST digits\n",
    "Reduction 60000 train + 10000 test images to 24000 train + 4000 test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set of 10 digit images, the same size as the set of letters (2400 trains + 400 tests per class)\n",
    "\n",
    "# the MNIST data was pre-divided into 10 classes\n",
    "path = f'./data/MNIST_0_1_2_3_4_5_6_7_8_9/'\n",
    "\n",
    "for i in classes: \n",
    "    globals()[f'train_digit_{i}_images'] = torch.load(f'{path}train_images_{i}.pt', map_location=torch.device('cpu'))[0:2400]\n",
    "    globals()[f'train_digit_{i}_labels'] = torch.load(f'{path}train_labels_{i}.pt', map_location=torch.device('cpu'))[0:2400]\n",
    "    globals()[f'test_digit_{i}_images'] = torch.load(f'{path}test_images_{i}.pt', map_location=torch.device('cpu'))[0:400]\n",
    "    globals()[f'test_digit_{i}_labels'] = torch.load(f'{path}test_labels_{i}.pt', map_location=torch.device('cpu'))[0:400]\n",
    "\n",
    "train_MNIST_labels = globals()[f'train_digit_0_labels']\n",
    "train_MNIST_images = globals()[f'train_digit_0_images']\n",
    "test_MNIST_labels = globals()[f'test_digit_0_labels']\n",
    "test_MNIST_images = globals()[f'test_digit_0_images']                                 \n",
    "\n",
    "for i in range(1, 10):\n",
    "    train_MNIST_labels = torch.cat((train_MNIST_labels, globals()[f'train_digit_{i}_labels']), 0)\n",
    "    train_MNIST_images = torch.cat((train_MNIST_images, globals()[f'train_digit_{i}_images']), 0)\n",
    "\n",
    "    test_MNIST_labels = torch.cat((test_MNIST_labels, globals()[f'test_digit_{i}_labels']), 0)\n",
    "    test_MNIST_images = torch.cat((test_MNIST_images, globals()[f'test_digit_{i}_images']), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([24000]), torch.Size([4000]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_MNIST_labels.size(), test_MNIST_labels.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Element permutation\n",
    "\n",
    "train_order = torch.randperm(train_MNIST_labels.shape[0])\n",
    "test_order = torch.randperm(test_MNIST_labels.shape[0])\n",
    "\n",
    "train_MNIST_labels = train_MNIST_labels[train_order].view(train_MNIST_labels.size())\n",
    "train_MNIST_images = train_MNIST_images[train_order].view(train_MNIST_images.size())\n",
    "\n",
    "test_MNIST_labels = test_MNIST_labels[test_order].view(test_MNIST_labels.size())\n",
    "test_MNIST_images = test_MNIST_images[test_order].view(test_MNIST_images.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_MNIST_set = CustomTensorDataset(tensors=(train_MNIST_images, train_MNIST_labels), transform=s1c1_PIL)\n",
    "test_MNIST_set = CustomTensorDataset(tensors=(test_MNIST_images, test_MNIST_labels), transform=s1c1_PIL)\n",
    "\n",
    "train_MNIST_loader = DataLoader(train_MNIST_set, batch_size=len(train_MNIST_set))\n",
    "test_MNIST_loader = DataLoader(test_MNIST_set, batch_size=len(test_MNIST_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mozafari = MozafariMNIST2018()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available\n"
     ]
    }
   ],
   "source": [
    "use_cuda = True\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "    device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MozafariMNIST2018(\n",
       "  (conv1): Convolution()\n",
       "  (conv2): Convolution()\n",
       "  (conv3): Convolution()\n",
       "  (stdp1): STDP(\n",
       "    (conv_layer): Convolution()\n",
       "  )\n",
       "  (stdp2): STDP(\n",
       "    (conv_layer): Convolution()\n",
       "  )\n",
       "  (stdp3): STDP(\n",
       "    (conv_layer): Convolution()\n",
       "  )\n",
       "  (anti_stdp3): STDP(\n",
       "    (conv_layer): Convolution()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if use_cuda:\n",
    "    mozafari.cuda()   \n",
    "#mozafari.to(device)\n",
    "mozafari.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial training on digits with radius inhibition r1=5, r2=2, r3=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the first layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the first layer\n",
      "Epoch 0\n",
      "0 images\n",
      "10000 images\n",
      "20000 images\n",
      "Iteration 0\n",
      "Done!\n",
      "Epoch 1\n",
      "30000 images\n",
      "40000 images\n",
      "Iteration 0\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print(\"Training the first layer\")\n",
    "\n",
    "for epoch in range(2):\n",
    "    print(\"Epoch\", epoch)\n",
    "    iter = 0\n",
    "    for data, targets in train_MNIST_loader:\n",
    "        print(\"Iteration\", iter)\n",
    "        train_unsupervise(mozafari, data, 1)\n",
    "        print(\"Done!\")\n",
    "        iter+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the second layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the second layer\n",
      "Epoch 0\n",
      "50000 images\n",
      "60000 images\n",
      "70000 images\n",
      "Iteration 0\n",
      "Done!\n",
      "Epoch 1\n",
      "80000 images\n",
      "90000 images\n",
      "Iteration 0\n",
      "Done!\n",
      "Epoch 2\n",
      "100000 images\n",
      "110000 images\n",
      "Iteration 0\n",
      "Done!\n",
      "Epoch 3\n",
      "120000 images\n",
      "130000 images\n",
      "140000 images\n",
      "Iteration 0\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print(\"Training the second layer\")\n",
    "\n",
    "for epoch in range(4):\n",
    "    print(\"Epoch\", epoch)\n",
    "    iter = 0\n",
    "    for data,targets in train_MNIST_loader:\n",
    "        print(\"Iteration\", iter)\n",
    "        train_unsupervise(mozafari, data, 2)\n",
    "        print(\"Done!\")\n",
    "        iter+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the third layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial adaptive learning rates\n",
    "\n",
    "apr = mozafari.stdp3.learning_rate[0][0].item()\n",
    "anr = mozafari.stdp3.learning_rate[0][1].item()\n",
    "app = mozafari.anti_stdp3.learning_rate[0][1].item()\n",
    "anp = mozafari.anti_stdp3.learning_rate[0][0].item()\n",
    "               \n",
    "parametr_set = pd.DataFrame(columns=['epoch', 'train', 'test', 'test_previous',   \n",
    "                                 'apr_adapt', 'anr_adapt', 'app_adapt', 'anp_adapt', \n",
    "                                 'stdp3.learning_rate[0]', 'stdp3.learning_rate[1]',\n",
    "                                 'anti_stdp3.learning_rate[0]', 'anti_stdp3.learning_rate[1]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight moving will be during 0 epochs\n",
      "--------------------------------------------------\n",
      "Epoch #:  0\n",
      "150000 images\n",
      "160000 images\n",
      "Current Train: 77.41%\n",
      "170000 images\n",
      "Current Test: 79.53%\n",
      "Operational time of epoch #0: 1 min 51 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  1\n",
      "180000 images\n",
      "190000 images\n",
      "Current Train: 88.16%\n",
      "Current Test: 87.92%\n",
      "Operational time of epoch #1: 1 min 51 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  2\n",
      "200000 images\n",
      "210000 images\n",
      "220000 images\n",
      "Current Train: 90.37%\n",
      "Current Test: 88.60%\n",
      "Operational time of epoch #2: 1 min 48 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  3\n",
      "230000 images\n",
      "240000 images\n",
      "250000 images\n",
      "Current Train: 91.42%\n",
      "Current Test: 90.28%\n",
      "Operational time of epoch #3: 1 min 49 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  4\n",
      "260000 images\n",
      "270000 images\n",
      "Current Train: 92.39%\n",
      "280000 images\n",
      "Current Test: 90.60%\n",
      "Operational time of epoch #4: 1 min 53 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  5\n",
      "290000 images\n",
      "300000 images\n",
      "Current Train: 93.18%\n",
      "310000 images\n",
      "Current Test: 91.07%\n",
      "Operational time of epoch #5: 1 min 51 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  6\n",
      "320000 images\n",
      "330000 images\n",
      "Current Train: 93.70%\n",
      "Current Test: 91.75%\n",
      "Operational time of epoch #6: 1 min 52 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  7\n",
      "340000 images\n",
      "350000 images\n",
      "360000 images\n",
      "Current Train: 93.90%\n",
      "Current Test: 91.88%\n",
      "Operational time of epoch #7: 1 min 52 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  8\n",
      "370000 images\n",
      "380000 images\n",
      "390000 images\n",
      "Current Train: 94.14%\n",
      "Current Test: 91.88%\n",
      "Operational time of epoch #8: 1 min 48 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  9\n",
      "400000 images\n",
      "410000 images\n",
      "Current Train: 94.55%\n",
      "420000 images\n",
      "Current Test: 92.15%\n",
      "Operational time of epoch #9: 1 min 50 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  10\n",
      "430000 images\n",
      "440000 images\n",
      "Current Train: 94.58%\n",
      "450000 images\n",
      "Current Test: 92.33%\n",
      "Operational time of epoch #10: 1 min 49 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  11\n",
      "460000 images\n",
      "470000 images\n",
      "Current Train: 94.75%\n",
      "Current Test: 92.38%\n",
      "Operational time of epoch #11: 1 min 53 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  12\n",
      "480000 images\n",
      "490000 images\n",
      "500000 images\n",
      "Current Train: 94.99%\n",
      "Current Test: 92.58%\n",
      "Operational time of epoch #12: 1 min 53 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  13\n",
      "510000 images\n",
      "520000 images\n",
      "530000 images\n",
      "Current Train: 95.08%\n",
      "Current Test: 92.65%\n",
      "Operational time of epoch #13: 1 min 52 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  14\n",
      "540000 images\n",
      "550000 images\n",
      "Current Train: 95.37%\n",
      "560000 images\n",
      "Current Test: 92.95%\n",
      "Operational time of epoch #14: 1 min 52 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  15\n",
      "570000 images\n",
      "580000 images\n",
      "Current Train: 95.42%\n",
      "590000 images\n",
      "Current Test: 93.03%\n",
      "Operational time of epoch #15: 1 min 53 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  16\n",
      "600000 images\n",
      "610000 images\n",
      "Current Train: 95.55%\n",
      "Current Test: 93.15%\n",
      "Operational time of epoch #16: 1 min 52 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  17\n",
      "620000 images\n",
      "630000 images\n",
      "640000 images\n",
      "Current Train: 95.63%\n",
      "Current Test: 92.88%\n",
      "Operational time of epoch #17: 1 min 50 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  18\n",
      "650000 images\n",
      "660000 images\n",
      "670000 images\n",
      "Current Train: 95.66%\n",
      "Current Test: 93.10%\n",
      "Operational time of epoch #18: 1 min 49 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  19\n",
      "680000 images\n",
      "690000 images\n",
      "Current Train: 95.86%\n",
      "700000 images\n",
      "Current Test: 93.25%\n",
      "Operational time of epoch #19: 1 min 50 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  20\n",
      "710000 images\n",
      "720000 images\n",
      "Current Train: 96.09%\n",
      "730000 images\n",
      "Current Test: 93.23%\n",
      "Operational time of epoch #20: 1 min 49 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  21\n",
      "740000 images\n",
      "750000 images\n",
      "Current Train: 96.00%\n",
      "Current Test: 93.27%\n",
      "Operational time of epoch #21: 1 min 49 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  22\n",
      "760000 images\n",
      "770000 images\n",
      "780000 images\n",
      "Current Train: 96.15%\n",
      "Current Test: 93.33%\n",
      "Operational time of epoch #22: 1 min 50 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  23\n",
      "790000 images\n",
      "800000 images\n",
      "810000 images\n",
      "Current Train: 96.17%\n",
      "Current Test: 93.35%\n",
      "Operational time of epoch #23: 1 min 52 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  24\n",
      "820000 images\n",
      "830000 images\n",
      "Current Train: 96.25%\n",
      "840000 images\n",
      "Current Test: 93.75%\n",
      "Operational time of epoch #24: 1 min 53 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  25\n",
      "850000 images\n",
      "860000 images\n",
      "Current Train: 96.31%\n",
      "870000 images\n",
      "Current Test: 93.70%\n",
      "Operational time of epoch #25: 1 min 51 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  26\n",
      "880000 images\n",
      "890000 images\n",
      "Current Train: 96.41%\n",
      "Current Test: 93.50%\n",
      "Operational time of epoch #26: 1 min 54 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  27\n",
      "900000 images\n",
      "910000 images\n",
      "920000 images\n",
      "Current Train: 96.39%\n",
      "Current Test: 93.75%\n",
      "Operational time of epoch #27: 1 min 54 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  28\n",
      "930000 images\n",
      "940000 images\n",
      "950000 images\n",
      "Current Train: 96.62%\n",
      "Current Test: 93.77%\n",
      "Operational time of epoch #28: 1 min 49 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  29\n",
      "960000 images\n",
      "970000 images\n",
      "Current Train: 96.56%\n",
      "980000 images\n",
      "Current Test: 93.70%\n",
      "Operational time of epoch #29: 1 min 49 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  30\n",
      "990000 images\n",
      "1000000 images\n",
      "Current Train: 96.60%\n",
      "1010000 images\n",
      "Current Test: 93.80%\n",
      "Operational time of epoch #30: 1 min 48 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  31\n",
      "1020000 images\n",
      "1030000 images\n",
      "Current Train: 96.62%\n",
      "Current Test: 93.97%\n",
      "Operational time of epoch #31: 1 min 49 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  32\n",
      "1040000 images\n",
      "1050000 images\n",
      "1060000 images\n",
      "Current Train: 96.71%\n",
      "Current Test: 93.90%\n",
      "Operational time of epoch #32: 1 min 51 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  33\n",
      "1070000 images\n",
      "1080000 images\n",
      "1090000 images\n",
      "Current Train: 96.65%\n",
      "Current Test: 93.83%\n",
      "Operational time of epoch #33: 1 min 52 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  34\n",
      "1100000 images\n",
      "1110000 images\n",
      "Current Train: 96.78%\n",
      "1120000 images\n",
      "Current Test: 93.88%\n",
      "Operational time of epoch #34: 1 min 54 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  35\n",
      "1130000 images\n",
      "1140000 images\n",
      "Current Train: 96.78%\n",
      "1150000 images\n",
      "Current Test: 93.90%\n",
      "Operational time of epoch #35: 1 min 50 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  36\n",
      "1160000 images\n",
      "1170000 images\n",
      "Current Train: 96.84%\n",
      "Current Test: 94.15%\n",
      "Operational time of epoch #36: 1 min 48 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  37\n",
      "1180000 images\n",
      "1190000 images\n",
      "1200000 images\n",
      "Current Train: 96.83%\n",
      "Current Test: 94.00%\n",
      "Operational time of epoch #37: 1 min 49 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  38\n",
      "1210000 images\n",
      "1220000 images\n",
      "1230000 images\n",
      "Current Train: 96.88%\n",
      "Current Test: 94.05%\n",
      "Operational time of epoch #38: 1 min 49 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  39\n",
      "1240000 images\n",
      "1250000 images\n",
      "Current Train: 97.09%\n",
      "1260000 images\n",
      "Current Test: 93.95%\n",
      "Operational time of epoch #39: 1 min 49 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  40\n",
      "1270000 images\n",
      "1280000 images\n",
      "Current Train: 97.00%\n",
      "1290000 images\n",
      "Current Test: 94.08%\n",
      "Operational time of epoch #40: 1 min 49 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  41\n",
      "1300000 images\n",
      "1310000 images\n",
      "Current Train: 97.04%\n",
      "Current Test: 94.12%\n",
      "Operational time of epoch #41: 1 min 51 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  42\n",
      "1320000 images\n",
      "1330000 images\n",
      "1340000 images\n",
      "Current Train: 97.10%\n",
      "Current Test: 94.15%\n",
      "Operational time of epoch #42: 1 min 48 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1350000 images\n",
      "1360000 images\n",
      "1370000 images\n",
      "Current Train: 97.01%\n",
      "Current Test: 94.15%\n",
      "Operational time of epoch #43: 1 min 48 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  44\n",
      "1380000 images\n",
      "1390000 images\n",
      "Current Train: 97.15%\n",
      "1400000 images\n",
      "Current Test: 94.17%\n",
      "Operational time of epoch #44: 1 min 48 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  45\n",
      "1410000 images\n",
      "1420000 images\n",
      "Current Train: 97.17%\n",
      "1430000 images\n",
      "Current Test: 94.25%\n",
      "Operational time of epoch #45: 1 min 50 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  46\n",
      "1440000 images\n",
      "1450000 images\n",
      "Current Train: 97.18%\n",
      "Current Test: 94.12%\n",
      "Operational time of epoch #46: 1 min 49 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  47\n",
      "1460000 images\n",
      "1470000 images\n",
      "1480000 images\n",
      "Current Train: 97.26%\n",
      "Current Test: 94.40%\n",
      "Operational time of epoch #47: 1 min 50 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  48\n",
      "1490000 images\n",
      "1500000 images\n",
      "1510000 images\n",
      "Current Train: 97.24%\n",
      "Current Test: 94.25%\n",
      "Operational time of epoch #48: 1 min 50 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  49\n",
      "1520000 images\n",
      "1530000 images\n",
      "Current Train: 97.32%\n",
      "1540000 images\n",
      "Current Test: 94.27%\n",
      "Operational time of epoch #49: 1 min 49 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  50\n",
      "1550000 images\n",
      "1560000 images\n",
      "Current Train: 97.35%\n",
      "1570000 images\n",
      "Current Test: 94.38%\n",
      "Operational time of epoch #50: 1 min 51 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  51\n",
      "1580000 images\n",
      "1590000 images\n",
      "Current Train: 97.36%\n",
      "Current Test: 94.33%\n",
      "Operational time of epoch #51: 1 min 52 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  52\n",
      "1600000 images\n",
      "1610000 images\n",
      "1620000 images\n",
      "Current Train: 97.36%\n",
      "Current Test: 94.40%\n",
      "Operational time of epoch #52: 1 min 53 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  53\n",
      "1630000 images\n",
      "1640000 images\n",
      "1650000 images\n",
      "Current Train: 97.49%\n",
      "Current Test: 94.53%\n",
      "Operational time of epoch #53: 1 min 52 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  54\n",
      "1660000 images\n",
      "1670000 images\n",
      "Current Train: 97.38%\n",
      "1680000 images\n",
      "Current Test: 94.30%\n",
      "Operational time of epoch #54: 1 min 52 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  55\n",
      "1690000 images\n",
      "1700000 images\n",
      "Current Train: 97.54%\n",
      "1710000 images\n",
      "Current Test: 94.35%\n",
      "Operational time of epoch #55: 1 min 50 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  56\n",
      "1720000 images\n",
      "1730000 images\n",
      "Current Train: 97.53%\n",
      "Current Test: 94.55%\n",
      "Operational time of epoch #56: 1 min 51 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  57\n",
      "1740000 images\n",
      "1750000 images\n",
      "1760000 images\n",
      "Current Train: 97.49%\n",
      "Current Test: 94.42%\n",
      "Operational time of epoch #57: 1 min 50 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  58\n",
      "1770000 images\n",
      "1780000 images\n",
      "1790000 images\n",
      "Current Train: 97.62%\n",
      "Current Test: 94.60%\n",
      "Operational time of epoch #58: 1 min 51 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  59\n",
      "1800000 images\n",
      "1810000 images\n",
      "Current Train: 97.65%\n",
      "1820000 images\n",
      "Current Test: 94.40%\n",
      "Operational time of epoch #59: 1 min 49 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  60\n",
      "1830000 images\n",
      "1840000 images\n",
      "Current Train: 97.58%\n",
      "1850000 images\n",
      "Current Test: 94.55%\n",
      "Operational time of epoch #60: 1 min 50 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  61\n",
      "1860000 images\n",
      "1870000 images\n",
      "Current Train: 97.69%\n",
      "Current Test: 94.55%\n",
      "Operational time of epoch #61: 1 min 50 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  62\n",
      "1880000 images\n",
      "1890000 images\n",
      "1900000 images\n",
      "Current Train: 97.78%\n",
      "Current Test: 94.55%\n",
      "Operational time of epoch #62: 1 min 52 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  63\n",
      "1910000 images\n",
      "1920000 images\n",
      "1930000 images\n",
      "Current Train: 97.74%\n",
      "Current Test: 94.53%\n",
      "Operational time of epoch #63: 1 min 50 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  64\n",
      "1940000 images\n",
      "1950000 images\n",
      "Current Train: 97.67%\n",
      "1960000 images\n",
      "Current Test: 94.53%\n",
      "Operational time of epoch #64: 1 min 51 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  65\n",
      "1970000 images\n",
      "1980000 images\n",
      "Current Train: 97.70%\n",
      "1990000 images\n",
      "Current Test: 94.42%\n",
      "Operational time of epoch #65: 1 min 50 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  66\n",
      "2000000 images\n",
      "2010000 images\n",
      "Current Train: 97.76%\n",
      "Current Test: 94.55%\n",
      "Operational time of epoch #66: 1 min 51 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  67\n",
      "2020000 images\n",
      "2030000 images\n",
      "2040000 images\n",
      "Current Train: 97.84%\n",
      "Current Test: 94.60%\n",
      "Operational time of epoch #67: 1 min 54 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  68\n",
      "2050000 images\n",
      "2060000 images\n",
      "2070000 images\n",
      "Current Train: 97.77%\n",
      "Current Test: 94.65%\n",
      "Operational time of epoch #68: 1 min 51 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  69\n",
      "2080000 images\n",
      "2090000 images\n",
      "Current Train: 97.82%\n",
      "2100000 images\n",
      "Current Test: 94.67%\n",
      "Operational time of epoch #69: 1 min 52 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  70\n",
      "2110000 images\n",
      "2120000 images\n",
      "Current Train: 97.87%\n",
      "2130000 images\n",
      "Current Test: 94.80%\n",
      "Operational time of epoch #70: 1 min 53 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  71\n",
      "2140000 images\n",
      "2150000 images\n",
      "Current Train: 97.87%\n",
      "Current Test: 94.77%\n",
      "Operational time of epoch #71: 1 min 51 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  72\n",
      "2160000 images\n",
      "2170000 images\n",
      "2180000 images\n",
      "Current Train: 97.97%\n",
      "Current Test: 94.80%\n",
      "Operational time of epoch #72: 1 min 52 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  73\n",
      "2190000 images\n",
      "2200000 images\n",
      "2210000 images\n",
      "Current Train: 97.93%\n",
      "Current Test: 94.85%\n",
      "Operational time of epoch #73: 1 min 51 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  74\n",
      "2220000 images\n",
      "2230000 images\n",
      "Current Train: 97.90%\n",
      "2240000 images\n",
      "Current Test: 94.70%\n",
      "Operational time of epoch #74: 1 min 52 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  75\n",
      "2250000 images\n",
      "2260000 images\n",
      "Current Train: 97.94%\n",
      "2270000 images\n",
      "Current Test: 94.88%\n",
      "Operational time of epoch #75: 1 min 50 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  76\n",
      "2280000 images\n",
      "2290000 images\n",
      "Current Train: 97.90%\n",
      "Current Test: 94.75%\n",
      "Operational time of epoch #76: 1 min 50 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  77\n",
      "2300000 images\n",
      "2310000 images\n",
      "2320000 images\n",
      "Current Train: 98.03%\n",
      "Current Test: 94.88%\n",
      "Operational time of epoch #77: 1 min 51 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  78\n",
      "2330000 images\n",
      "2340000 images\n",
      "2350000 images\n",
      "Current Train: 98.05%\n",
      "Current Test: 94.92%\n",
      "Operational time of epoch #78: 1 min 48 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  79\n",
      "2360000 images\n",
      "2370000 images\n",
      "Current Train: 98.04%\n",
      "2380000 images\n",
      "Current Test: 94.88%\n",
      "Operational time of epoch #79: 1 min 47 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  80\n",
      "2390000 images\n",
      "2400000 images\n",
      "Current Train: 97.95%\n",
      "2410000 images\n",
      "Current Test: 94.73%\n",
      "Operational time of epoch #80: 1 min 51 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  81\n",
      "2420000 images\n",
      "2430000 images\n",
      "Current Train: 97.98%\n",
      "Current Test: 94.95%\n",
      "Operational time of epoch #81: 1 min 49 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  82\n",
      "2440000 images\n",
      "2450000 images\n",
      "2460000 images\n",
      "Current Train: 98.00%\n",
      "Current Test: 94.85%\n",
      "Operational time of epoch #82: 1 min 50 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  83\n",
      "2470000 images\n",
      "2480000 images\n",
      "2490000 images\n",
      "Current Train: 97.99%\n",
      "Current Test: 95.10%\n",
      "Operational time of epoch #83: 1 min 50 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  84\n",
      "2500000 images\n",
      "2510000 images\n",
      "Current Train: 98.14%\n",
      "2520000 images\n",
      "Current Test: 94.97%\n",
      "Operational time of epoch #84: 1 min 48 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  85\n",
      "2530000 images\n",
      "2540000 images\n",
      "Current Train: 98.08%\n",
      "2550000 images\n",
      "Current Test: 94.97%\n",
      "Operational time of epoch #85: 1 min 50 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2560000 images\n",
      "2570000 images\n",
      "Current Train: 98.14%\n",
      "Current Test: 94.85%\n",
      "Operational time of epoch #86: 1 min 50 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  87\n",
      "2580000 images\n",
      "2590000 images\n",
      "2600000 images\n",
      "Current Train: 98.15%\n",
      "Current Test: 94.97%\n",
      "Operational time of epoch #87: 1 min 51 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  88\n",
      "2610000 images\n",
      "2620000 images\n",
      "2630000 images\n",
      "Current Train: 98.18%\n",
      "Current Test: 94.95%\n",
      "Operational time of epoch #88: 1 min 50 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  89\n",
      "2640000 images\n",
      "2650000 images\n",
      "Current Train: 98.09%\n",
      "2660000 images\n",
      "Current Test: 95.17%\n",
      "Operational time of epoch #89: 1 min 55 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  90\n",
      "2670000 images\n",
      "2680000 images\n",
      "Current Train: 98.15%\n",
      "2690000 images\n",
      "Current Test: 94.92%\n",
      "Operational time of epoch #90: 1 min 54 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  91\n",
      "2700000 images\n",
      "2710000 images\n",
      "Current Train: 98.21%\n",
      "Current Test: 94.97%\n",
      "Operational time of epoch #91: 1 min 53 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  92\n",
      "2720000 images\n",
      "2730000 images\n",
      "2740000 images\n",
      "Current Train: 98.19%\n",
      "Current Test: 95.03%\n",
      "Operational time of epoch #92: 1 min 49 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  93\n",
      "2750000 images\n",
      "2760000 images\n",
      "2770000 images\n",
      "Current Train: 98.17%\n",
      "Current Test: 95.08%\n",
      "Operational time of epoch #93: 1 min 48 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  94\n",
      "2780000 images\n",
      "2790000 images\n",
      "Current Train: 98.15%\n",
      "2800000 images\n",
      "Current Test: 95.20%\n",
      "Operational time of epoch #94: 1 min 48 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  95\n",
      "2810000 images\n",
      "2820000 images\n",
      "Current Train: 98.17%\n",
      "2830000 images\n",
      "Current Test: 95.03%\n",
      "Operational time of epoch #95: 1 min 48 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  96\n",
      "2840000 images\n",
      "2850000 images\n",
      "Current Train: 98.20%\n",
      "Current Test: 95.10%\n",
      "Operational time of epoch #96: 1 min 48 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  97\n",
      "2860000 images\n",
      "2870000 images\n",
      "2880000 images\n",
      "Current Train: 98.23%\n",
      "Current Test: 94.95%\n",
      "Operational time of epoch #97: 1 min 48 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  98\n",
      "2890000 images\n",
      "2900000 images\n",
      "2910000 images\n",
      "Current Train: 98.28%\n",
      "Current Test: 95.15%\n",
      "Operational time of epoch #98: 1 min 48 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  99\n",
      "2920000 images\n",
      "2930000 images\n",
      "Current Train: 98.22%\n",
      "2940000 images\n",
      "Current Test: 95.10%\n",
      "Operational time of epoch #99: 1 min 48 sec\n",
      "==================== SUMMARY ====================\n",
      "        Best Train: 98.28% on 98 epoch\n",
      "         Best Test: 95.20% on 94 epoch\n",
      "Best Test Previous: 0.00% on 0 epoch\n"
     ]
    }
   ],
   "source": [
    "# training the 3rd layer\n",
    "\n",
    "first_test = third_layer(file_name_net=\"saved_24000_digits_(radius_inh_1)_0.net\",\n",
    "                        file_name_csv='parameter_set_24000_digits_(radius_inh_1)_0.csv',\n",
    "                        adaptive_int=1, previous_epochs=0, epochs=100, \n",
    "                        train_loader=train_MNIST_loader, \n",
    "                        test_loader=test_MNIST_loader, \n",
    "                        test_previous_loader=[],\n",
    "                        model=mozafari, apr=apr, anr=anr, app=app, anp=anp, \n",
    "                        parametr_set=parametr_set, steps=0, percent=0, it_continues=False)\n",
    "\n",
    "parametr_set = first_test[0] \n",
    "counter = first_test[1] \n",
    "previous_epochs = first_test[2]\n",
    "apr = first_test[3] \n",
    "anr = first_test[4] \n",
    "app = first_test[5] \n",
    "anp = first_test[6]\n",
    "conv3_data_train = first_test[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "curve_graph(parametr_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
